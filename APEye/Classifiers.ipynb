{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "U2_kgO6EVly4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install gensim library\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-T04SZTwrzdT",
        "outputId": "54087c88-689c-425c-ed43-3bcd3b2bb13f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.9.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.17.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.2.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "y2ku50G1xvfX"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from google.colab import drive\n",
        "import random\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define project path, example\n",
        "PATH=\"/content/drive/MyDrive/CS171Project\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSGeaDEEwpPC",
        "outputId": "a0723b8f-ff0d-4643-b4b4-5f64ead5f72f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {
        "id": "o51DyhdqIqji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLgBwVyVjG7F"
      },
      "source": [
        "## Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MHd-7jlUo81O"
      },
      "outputs": [],
      "source": [
        "# Load all API calls\n",
        "with open(PATH + \"/data/APICalls.txt\", \"r\") as f:\n",
        "    api_calls = [line.strip() for line in f]\n",
        "\n",
        "# Encode each API call to an integer\n",
        "encoded_api_calls = {api_call: idx + 1 for idx, api_call in enumerate(api_calls)}\n",
        "\n",
        "# Map each family to an integer\n",
        "encoded_malware_family = {\n",
        "    \"adload\": 0,\n",
        "    \"bancos\": 1,\n",
        "    \"onlinegames\": 2,\n",
        "    \"vbinject\": 3,\n",
        "    \"vundo\": 4,\n",
        "    \"winwebsec\": 5,\n",
        "    \"zwangi\": 6\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_wolC2HtFzX"
      },
      "outputs": [],
      "source": [
        "# Path to folder containing malware families\n",
        "root_dir = PATH + \"/data/malwares\"\n",
        "\n",
        "encoded_dataset = []  # List of encoded sequences object\n",
        "vector_dataset = []   # List of vectorized sequences object\n",
        "\n",
        "# Loop through each malware family\n",
        "for family_name in os.listdir(root_dir):\n",
        "\n",
        "    # Loop through each file in malware family\n",
        "    family_path = os.path.join(root_dir, family_name)\n",
        "    for file_name in os.listdir(family_path):\n",
        "\n",
        "        # Open file and get its list of API calls\n",
        "        file_path = os.path.join(family_path, file_name)\n",
        "        with open(file_path, \"r\") as f:\n",
        "            sequence = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        # Encode the file's list of API calls\n",
        "        encoded_sequence = [encoded_api_calls[api] for api in sequence\n",
        "                            if api in encoded_api_calls]\n",
        "\n",
        "        # Add the encoded list and its family as an object\n",
        "        encoded_dataset.append({\n",
        "            \"family\": encoded_malware_family[family_name],\n",
        "            \"sequence\": encoded_sequence\n",
        "        })\n",
        "\n",
        "        # Add plain the unencoded list API calls for different preprocessing\n",
        "        vector_dataset.append({\n",
        "            \"family\": encoded_malware_family[family_name],\n",
        "            \"sequence\": sequence\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get ideal length of sequence (leave out outliers)\n",
        "file_lengths = [len(sample[\"sequence\"]) for sample in encoded_dataset]\n",
        "max_length = int(np.percentile(file_lengths, 90))\n",
        "\n",
        "# Path for storing lists\n",
        "os.makedirs(PATH + \"/compiled\", exist_ok=True)"
      ],
      "metadata": {
        "id": "HDmrmWfBvtuJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoded Dataset"
      ],
      "metadata": {
        "id": "tHCbl-h-HGSF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "FwrpmIkRxRFl"
      },
      "outputs": [],
      "source": [
        "# Truncate or pad sequences based on ideal length\n",
        "def pad_or_truncate(seq, max_length):\n",
        "    if len(seq) > max_length:\n",
        "        return seq[:max_length] # Truncate\n",
        "    else:\n",
        "        return seq + [0]* (max_length - len(seq)) # Pad with 0's\n",
        "\n",
        "for sample in encoded_dataset:\n",
        "    sample[\"sequence\"] = pad_or_truncate(sample[\"sequence\"], max_length)\n",
        "\n",
        "# Shuffle samples and save\n",
        "random.shuffle(encoded_dataset)\n",
        "\n",
        "# Store encoded dataset\n",
        "with open(PATH + \"/compiled/encoded_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump(encoded_dataset, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Dataset"
      ],
      "metadata": {
        "id": "BjOdqGyfHMAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract API call sequences to a list\n",
        "sequences = [sample[\"sequence\"] for sample in vector_dataset]\n",
        "\n",
        "# Train the Word2Vec model on API call sequences\n",
        "word2vec_model = Word2Vec(sequences, size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNn9Q2kXsBvt",
        "outputId": "041124a9-158f-4034-c2b1-79855450bfda"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gensim/models/base_any2vec.py:742: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding dimension\n",
        "embedding_dim = word2vec_model.vector_size\n",
        "\n",
        "# Function for converting an API call sequence to word2vec embeddings, pad as needed\n",
        "def sequence_to_matrix(seq, model, max_length):\n",
        "    mat = []\n",
        "    for api in seq[:max_length]:\n",
        "        if api in model.wv:\n",
        "            mat.append(model.wv[api])\n",
        "        else:\n",
        "            mat.append(np.zeros(embedding_dim))\n",
        "    while len(mat) < max_length:\n",
        "        mat.append(np.zeros(embedding_dim))\n",
        "    return np.stack(mat)\n",
        "\n",
        "# For each sample, convert that sample's sequence into a matrix of embeddings\n",
        "vector_data = [\n",
        "    (sequence_to_matrix(sample[\"sequence\"], word2vec_model, max_length), sample[\"family\"])\n",
        "    for sample in vector_dataset\n",
        "]"
      ],
      "metadata": {
        "id": "r3p7AA_IvmCw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle samples and save\n",
        "random.shuffle(vector_data)\n",
        "X_mat = np.stack([x for x, _ in vector_data])\n",
        "y     = np.array([y for _, y in vector_data])\n",
        "\n",
        "with open(PATH + \"/compiled/vector_dataset.pkl\", \"wb\") as f:\n",
        "    pickle.dump((X_mat, y), f)"
      ],
      "metadata": {
        "id": "QRx0wQhpxeHY"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Processed Data"
      ],
      "metadata": {
        "id": "KFPIGsOIJONr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load encoded dataset\n",
        "with open(PATH + \"/compiled/encoded_dataset.pkl\", \"rb\") as f:\n",
        "    encoded_dataset = pickle.load(f)\n",
        "\n",
        "# Features and targets\n",
        "X_encoded = [sample[\"sequence\"] for sample in encoded_dataset]\n",
        "y_encoded = [sample[\"family\"] for sample in encoded_dataset]\n",
        "\n",
        "# Get sequence length\n",
        "sequence_length = len(X_encoded[0])\n",
        "\n",
        "# Reshape to (samples, timesteps, features)\n",
        "X_encoded = np.array(X_encoded).reshape(len(X_encoded), sequence_length, 1)\n",
        "\n",
        "# Split data into training, validation, and testing\n",
        "X_train_encoded, X_temp, y_train_encoded, y_temp = train_test_split(\n",
        "    X_encoded, y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "X_val_encoded, X_test_encoded, y_val_encoded, y_test_encoded = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_encoded = tf.keras.utils.to_categorical(y_train_encoded)\n",
        "y_val_encoded = tf.keras.utils.to_categorical(y_val_encoded)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test_encoded)"
      ],
      "metadata": {
        "id": "IDVMH7xGJTr2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vector dataset\n",
        "with open(PATH + \"/compiled/vector_dataset.pkl\", \"rb\") as f:\n",
        "    X_mat, y = pickle.load(f)\n",
        "\n",
        "# Split data into training, validation, and testing\n",
        "y_cat = tf.keras.utils.to_categorical(y, num_classes=7)\n",
        "X_train_vector, X_temp, y_train_vector, y_temp = train_test_split(\n",
        "    X_mat, y_cat,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_cat\n",
        ")\n",
        "X_val_vector, X_test_vector, y_val_vector, y_test_vector = train_test_split(\n",
        "    X_temp, y_temp,\n",
        "    test_size=0.5,\n",
        "    random_state=42,\n",
        "    stratify=y_temp\n",
        ")"
      ],
      "metadata": {
        "id": "GIs9RX-URbuz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define CNN, RNN, LSTM"
      ],
      "metadata": {
        "id": "zFOT4A-gb81V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for creating a CNN model\n",
        "def create_cnn_model(input_shape, num_classes):\n",
        "    # Define model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv1D(256,                         # Number of features/patterns\n",
        "                               kernel_size=16,              # Sliding window size\n",
        "                               activation='relu',\n",
        "                               input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=4),          # Pooling, make samples generic -> less overfitting\n",
        "        tf.keras.layers.Flatten(),                          # Turns to 1D array so that dense layer can take it\n",
        "        tf.keras.layers.Dense(256, activation='relu'),      # Hidden layer\n",
        "        tf.keras.layers.Dropout(0.2),                       # Drops every 5th sample, forces model to learn without it\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "Tnzv6J4TIuJz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for creating a RNN model\n",
        "def create_rnn_model(input_shape, num_classes):\n",
        "    # Define model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.SimpleRNN(\n",
        "            256,\n",
        "            input_shape=input_shape,\n",
        "            return_sequences=False\n",
        "        ),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(256, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "XZN8icB8a1VC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for creating an LSTM model\n",
        "def create_lstm_model(input_shape, num_classes):\n",
        "    # Define model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.LSTM(256, input_shape=input_shape, return_sequences=False),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "L2_XvPogbQ2A"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train and Test Models"
      ],
      "metadata": {
        "id": "RvyrmmG0bf1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for training and evaluating neural network\n",
        "def train_and_evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    # Condition for early stopping\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                                  patience=3,\n",
        "                                                  restore_best_weights=True)\n",
        "    # Fit the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        validation_data=(X_val, y_val),\n",
        "        callbacks=[early_stop]\n",
        "    )\n",
        "\n",
        "    # Print accuracies\n",
        "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "    print(f\"Training Accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
        "    print(f\"Testing Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "BHxe61McbiV2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Encoded Dataset"
      ],
      "metadata": {
        "id": "AAiYCrAYbvRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of encoded features and labels\n",
        "input_shape_encoded = X_train_encoded.shape[1:]\n",
        "num_classes_encoded = y_train_encoded.shape[1]"
      ],
      "metadata": {
        "id": "_E6CT9P6cXY0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with encoded dataset\n",
        "encoded_cnn_model = create_cnn_model(input_shape_encoded, num_classes_encoded)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    encoded_cnn_model,\n",
        "    X_train_encoded, y_train_encoded,\n",
        "    X_val_encoded, y_val_encoded,\n",
        "    X_test_encoded, y_test_encoded\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2dl-cLsQyNu",
        "outputId": "ba2e6751-0479-4758-a808-d276ea2aa49e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 321ms/step - accuracy: 0.3551 - loss: 7.3346 - val_accuracy: 0.4211 - val_loss: 2.1599\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 277ms/step - accuracy: 0.6240 - loss: 1.3908 - val_accuracy: 0.6316 - val_loss: 0.9044\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 307ms/step - accuracy: 0.8370 - loss: 0.6248 - val_accuracy: 0.7895 - val_loss: 0.5698\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 266ms/step - accuracy: 0.8946 - loss: 0.3149 - val_accuracy: 0.8421 - val_loss: 0.4766\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 251ms/step - accuracy: 0.9627 - loss: 0.1367 - val_accuracy: 0.8158 - val_loss: 0.4834\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 263ms/step - accuracy: 0.9867 - loss: 0.0776 - val_accuracy: 0.8158 - val_loss: 0.5611\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 409ms/step - accuracy: 0.9971 - loss: 0.0389 - val_accuracy: 0.8421 - val_loss: 0.5708\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8427 - loss: 0.4902\n",
            "Training Accuracy: 0.9934\n",
            "Testing Accuracy: 0.8421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN with encoded dataset\n",
        "encoded_rnn_model = create_rnn_model(input_shape_encoded, num_classes_encoded)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    encoded_rnn_model,\n",
        "    X_train_encoded, y_train_encoded,\n",
        "    X_val_encoded, y_val_encoded,\n",
        "    X_test_encoded, y_test_encoded\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgfb0P__XDpv",
        "outputId": "f924a1d5-b97b-4a7c-9595-591424065007"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 572ms/step - accuracy: 0.2164 - loss: 1.9315 - val_accuracy: 0.2368 - val_loss: 1.8064\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 532ms/step - accuracy: 0.1920 - loss: 1.8147 - val_accuracy: 0.2632 - val_loss: 1.6829\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 685ms/step - accuracy: 0.2953 - loss: 1.7122 - val_accuracy: 0.2632 - val_loss: 1.6776\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 547ms/step - accuracy: 0.2541 - loss: 1.7224 - val_accuracy: 0.2632 - val_loss: 1.6838\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 806ms/step - accuracy: 0.2084 - loss: 1.7920 - val_accuracy: 0.2632 - val_loss: 1.6611\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 618ms/step - accuracy: 0.2914 - loss: 1.7144 - val_accuracy: 0.2632 - val_loss: 1.6536\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 555ms/step - accuracy: 0.2353 - loss: 1.7180 - val_accuracy: 0.2632 - val_loss: 1.6513\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 614ms/step - accuracy: 0.2497 - loss: 1.7221 - val_accuracy: 0.2632 - val_loss: 1.6530\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 796ms/step - accuracy: 0.2390 - loss: 1.6986 - val_accuracy: 0.2632 - val_loss: 1.6457\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 535ms/step - accuracy: 0.2480 - loss: 1.7317 - val_accuracy: 0.2632 - val_loss: 1.6401\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 542ms/step - accuracy: 0.2698 - loss: 1.6732 - val_accuracy: 0.2632 - val_loss: 1.6419\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 605ms/step - accuracy: 0.2504 - loss: 1.6820 - val_accuracy: 0.2632 - val_loss: 1.6364\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 781ms/step - accuracy: 0.2258 - loss: 1.6868 - val_accuracy: 0.2632 - val_loss: 1.6321\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 573ms/step - accuracy: 0.2260 - loss: 1.6791 - val_accuracy: 0.2632 - val_loss: 1.6361\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 604ms/step - accuracy: 0.2597 - loss: 1.6539 - val_accuracy: 0.2895 - val_loss: 1.6440\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 554ms/step - accuracy: 0.2374 - loss: 1.6933 - val_accuracy: 0.2632 - val_loss: 1.6390\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.3427 - loss: 1.5415 \n",
            "Training Accuracy: 0.2211\n",
            "Testing Accuracy: 0.3421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with encoded dataset\n",
        "encoded_lstm_model = create_lstm_model(input_shape_encoded, num_classes_encoded)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    encoded_lstm_model,\n",
        "    X_train_encoded, y_train_encoded,\n",
        "    X_val_encoded, y_val_encoded,\n",
        "    X_test_encoded, y_test_encoded\n",
        ")"
      ],
      "metadata": {
        "id": "Rgbs83e6eqBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34cdd7cc-c659-4c3c-9006-7899b4ef967e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.1478 - loss: 1.9139 - val_accuracy: 0.2632 - val_loss: 1.8133\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.2574 - loss: 1.8450 - val_accuracy: 0.2632 - val_loss: 1.7955\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.2787 - loss: 1.8033 - val_accuracy: 0.2632 - val_loss: 1.7340\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.2535 - loss: 1.7280 - val_accuracy: 0.2632 - val_loss: 1.6518\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.2438 - loss: 1.6964 - val_accuracy: 0.2632 - val_loss: 1.6587\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.2415 - loss: 1.6725 - val_accuracy: 0.2632 - val_loss: 1.6456\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.2506 - loss: 1.6515 - val_accuracy: 0.2632 - val_loss: 1.6456\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2s/step - accuracy: 0.2635 - loss: 1.6436 - val_accuracy: 0.2632 - val_loss: 1.6393\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.2455 - loss: 1.6534 - val_accuracy: 0.2632 - val_loss: 1.6473\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2s/step - accuracy: 0.2598 - loss: 1.6521 - val_accuracy: 0.2632 - val_loss: 1.6481\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.2791 - loss: 1.6441 - val_accuracy: 0.2632 - val_loss: 1.6196\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2s/step - accuracy: 0.2878 - loss: 1.6302 - val_accuracy: 0.2632 - val_loss: 1.6479\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 2s/step - accuracy: 0.2827 - loss: 1.6358 - val_accuracy: 0.2632 - val_loss: 1.6384\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.2444 - loss: 1.6447 - val_accuracy: 0.2632 - val_loss: 1.6480\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 239ms/step - accuracy: 0.3427 - loss: 1.5020\n",
            "Training Accuracy: 0.2541\n",
            "Testing Accuracy: 0.3421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Vector Dataset"
      ],
      "metadata": {
        "id": "CVa6aQRfcv-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shapes of vectorized features and labels\n",
        "input_shape_vector = X_train_vector.shape[1:]  # (max_length, embedding_dim)\n",
        "num_classes_vector = y_train_vector.shape[1]   # 7"
      ],
      "metadata": {
        "id": "6oLvHMfndQFK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with vector dataset\n",
        "vector_cnn_model = create_cnn_model(input_shape_vector, num_classes_vector)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    vector_cnn_model,\n",
        "    X_train_vector, y_train_vector,\n",
        "    X_val_vector, y_val_vector,\n",
        "    X_test_vector, y_test_vector\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iycjhkCES4D1",
        "outputId": "3a5b816e-7222-414e-f040-67bc7ad7cd21"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 944ms/step - accuracy: 0.3681 - loss: 3.6655 - val_accuracy: 0.6316 - val_loss: 1.2600\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.7029 - loss: 0.8441 - val_accuracy: 0.7368 - val_loss: 0.8425\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.8458 - loss: 0.4936 - val_accuracy: 0.7368 - val_loss: 1.1296\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.8911 - loss: 0.3413 - val_accuracy: 0.7368 - val_loss: 0.8641\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 985ms/step - accuracy: 0.8913 - loss: 0.2611 - val_accuracy: 0.7105 - val_loss: 1.0813\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 123ms/step - accuracy: 0.6678 - loss: 0.8039\n",
            "Training Accuracy: 0.8977\n",
            "Testing Accuracy: 0.6579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN with vector dataset\n",
        "vector_rnn_model = create_rnn_model(input_shape_vector, num_classes_vector)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    vector_rnn_model,\n",
        "    X_train_vector, y_train_vector,\n",
        "    X_val_vector, y_val_vector,\n",
        "    X_test_vector, y_test_vector\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NF-HXaSDdfm8",
        "outputId": "a91ad65d-8134-46fd-fbca-4afaac4ba483"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.2000 - loss: 1.8818 - val_accuracy: 0.2368 - val_loss: 1.8492\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 899ms/step - accuracy: 0.2668 - loss: 1.7296 - val_accuracy: 0.2632 - val_loss: 1.8539\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 654ms/step - accuracy: 0.2363 - loss: 1.7768 - val_accuracy: 0.3158 - val_loss: 1.7331\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 886ms/step - accuracy: 0.2294 - loss: 1.6771 - val_accuracy: 0.2632 - val_loss: 1.7599\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 817ms/step - accuracy: 0.1990 - loss: 1.7090 - val_accuracy: 0.2895 - val_loss: 1.6965\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 670ms/step - accuracy: 0.2663 - loss: 1.6447 - val_accuracy: 0.2368 - val_loss: 1.7066\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 709ms/step - accuracy: 0.3430 - loss: 1.6471 - val_accuracy: 0.2105 - val_loss: 1.7489\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 896ms/step - accuracy: 0.2996 - loss: 1.6294 - val_accuracy: 0.2895 - val_loss: 1.7355\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.2867 - loss: 1.7447\n",
            "Training Accuracy: 0.2970\n",
            "Testing Accuracy: 0.2895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM with vector dataset\n",
        "vector_lstm_model = create_lstm_model(input_shape_vector, num_classes_vector)\n",
        "\n",
        "train_and_evaluate_model(\n",
        "    vector_lstm_model,\n",
        "    X_train_vector, y_train_vector,\n",
        "    X_val_vector, y_val_vector,\n",
        "    X_test_vector, y_test_vector\n",
        ")"
      ],
      "metadata": {
        "id": "Bn54I_sIerzH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e324ad-990a-4c5f-beba-b668a664da64"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 2s/step - accuracy: 0.2009 - loss: 1.8825 - val_accuracy: 0.2368 - val_loss: 1.8423\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.2615 - loss: 1.7870 - val_accuracy: 0.2368 - val_loss: 1.8269\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3s/step - accuracy: 0.2808 - loss: 1.7238 - val_accuracy: 0.2895 - val_loss: 1.7130\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.2334 - loss: 1.6036 - val_accuracy: 0.2368 - val_loss: 1.7002\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.2714 - loss: 1.6095 - val_accuracy: 0.2895 - val_loss: 1.7010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 4s/step - accuracy: 0.2761 - loss: 1.5862 - val_accuracy: 0.2368 - val_loss: 1.7277\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - accuracy: 0.3007 - loss: 1.5773 - val_accuracy: 0.2895 - val_loss: 1.7340\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 262ms/step - accuracy: 0.2308 - loss: 1.7214\n",
            "Training Accuracy: 0.2937\n",
            "Testing Accuracy: 0.2368\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traditional Models"
      ],
      "metadata": {
        "id": "GM-8qRCzU0Zq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Data"
      ],
      "metadata": {
        "id": "BkPEDAANOB3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load all API calls\n",
        "with open(PATH + \"/data/APICalls.txt\", \"r\") as f:\n",
        "    api_calls = [line.strip() for line in f]\n",
        "\n",
        "# Create mapping of API call to index\n",
        "api_index_map = {api: idx for idx, api in enumerate(api_calls)}\n",
        "\n",
        "# Malware family encoding\n",
        "encoded_malware_family = {\n",
        "    \"adload\": 0,\n",
        "    \"bancos\": 1,\n",
        "    \"onlinegames\": 2,\n",
        "    \"vbinject\": 3,\n",
        "    \"vundo\": 4,\n",
        "    \"winwebsec\": 5,\n",
        "    \"zwangi\": 6\n",
        "}\n",
        "\n",
        "# Root directory containing malware families\n",
        "root_dir = PATH + \"/data/malwares\"\n",
        "\n",
        "# List for feature vectors and labels\n",
        "feature_vectors = []\n",
        "labels = []\n",
        "\n",
        "# Go through each malware family directory\n",
        "for family_name in os.listdir(root_dir):\n",
        "\n",
        "    # Loop through each malware file\n",
        "    family_path = os.path.join(root_dir, family_name)\n",
        "    for file_name in os.listdir(family_path):\n",
        "        file_path = os.path.join(family_path, file_name)\n",
        "\n",
        "        # Initialize zero vector for this file\n",
        "        vector = np.zeros(len(api_calls), dtype=int)\n",
        "\n",
        "        # Read file and count API calls\n",
        "        with open(file_path, \"r\") as f:\n",
        "            for line in f:\n",
        "                api = line.strip()\n",
        "                if api in api_index_map:\n",
        "                    vector[api_index_map[api]] += 1\n",
        "\n",
        "        # Append vector and label\n",
        "        feature_vectors.append(vector)\n",
        "        labels.append(encoded_malware_family[family_name])\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(feature_vectors, columns=api_calls)\n",
        "df[\"label\"] = labels\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(PATH + \"/compiled/api_call_counts.csv\", index=False)"
      ],
      "metadata": {
        "id": "QmdcnX7W4mP9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read Processed Data"
      ],
      "metadata": {
        "id": "rsKCoodyQXi5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV\n",
        "df = pd.read_csv(PATH + \"/compiled/api_call_counts.csv\")\n",
        "\n",
        "# Separate features and labels\n",
        "X = df.drop(\"label\", axis=1)\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n"
      ],
      "metadata": {
        "id": "MwTqunw95Rs9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "bgRhllb5OKrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM pipeline\n",
        "svm_pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(random_state=42)\n",
        ")\n",
        "\n",
        "# Grid search parameters\n",
        "svm_param_grid = {\n",
        "    'svc__kernel': ['linear'],\n",
        "    'svc__C': list(np.arange(5, 10, 0.01)),\n",
        "    'svc__class_weight': [None, 'balanced'],\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "svm_grid_search = GridSearchCV(svm_pipeline, svm_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "svm_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_svm_model = svm_grid_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters found:\", svm_grid_search.best_params_)\n",
        "print(f\"Training Accuracy: {best_svm_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"Testing Accuracy:  {best_svm_model.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "devaL3DJU3Xh",
        "outputId": "8690f2cc-a34e-49e6-f60f-1fa6db5d2a30"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 1000 candidates, totalling 5000 fits\n",
            "Best parameters found: {'svc__C': 7.549999999999946, 'svc__class_weight': None, 'svc__kernel': 'linear'}\n",
            "Training Accuracy: 0.9822\n",
            "Testing Accuracy:  0.8990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest Classifier"
      ],
      "metadata": {
        "id": "r6HJtMZBOPWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RFC pipeline\n",
        "rfc_pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    RandomForestClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "# Grid search parameters\n",
        "rfc_param_grid = {\n",
        "    'randomforestclassifier__n_estimators': list(np.arange(250, 271, 2)),\n",
        "    'randomforestclassifier__max_depth': list(np.arange(10, 21, 1)),\n",
        "    'randomforestclassifier__min_samples_split': [2],\n",
        "    'randomforestclassifier__min_samples_leaf': [1],\n",
        "    'randomforestclassifier__class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "rfc_grid_search = GridSearchCV(rfc_pipeline, rfc_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "rfc_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_rfc_model = rfc_grid_search.best_estimator_\n",
        "\n",
        "print(\"Best parameters found:\", rfc_grid_search.best_params_)\n",
        "print(f\"Training Accuracy: {best_rfc_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"Testing Accuracy:  {best_rfc_model.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcW4L6j6WG65",
        "outputId": "9646c718-e079-406d-ae68-edf56217f4a0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 121 candidates, totalling 605 fits\n",
            "Best parameters found: {'randomforestclassifier__class_weight': 'balanced', 'randomforestclassifier__max_depth': 16, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 258}\n",
            "Training Accuracy: 0.9975\n",
            "Testing Accuracy:  0.9495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "Ck-lkrs6Wv75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression pipeline\n",
        "logreg_pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression(max_iter=1000, random_state=42)\n",
        ")\n",
        "\n",
        "# Grid search parameters\n",
        "logreg_param_grid = {\n",
        "    'logisticregression__C': list(np.arange(0.1, 3.1, 0.2)),\n",
        "    'logisticregression__penalty': ['l2'],\n",
        "    'logisticregression__class_weight': ['balanced'],\n",
        "    'logisticregression__solver': ['lbfgs', 'newton-cg']\n",
        "}\n",
        "\n",
        "# Grid search with 5-fold cross-validation\n",
        "logreg_grid_search = GridSearchCV(logreg_pipeline, logreg_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "logreg_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_logreg_model = logreg_grid_search.best_estimator_\n",
        "\n",
        "# Accuracy\n",
        "print(\"Best parameters found:\", logreg_grid_search.best_params_)\n",
        "print(f\"Training Accuracy: {best_logreg_model.score(X_train, y_train):.4f}\")\n",
        "print(f\"Testing Accuracy:  {best_logreg_model.score(X_test, y_test):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW5THCWxVPID",
        "outputId": "41b8b346-7283-4760-ce09-d9bee50d2b4c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
            "Best parameters found: {'logisticregression__C': 1.1000000000000003, 'logisticregression__class_weight': 'balanced', 'logisticregression__penalty': 'l2', 'logisticregression__solver': 'lbfgs'}\n",
            "Training Accuracy: 0.9517\n",
            "Testing Accuracy:  0.9091\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}